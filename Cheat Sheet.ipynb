{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ed Bharucha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Chris Albon - Tips & Tricks](https://chrisalbon.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* .: Wilcard match single character\n",
    "* ^: Start of string\n",
    "* $: End of string\n",
    "* []: Matches one of the set of characters in []\n",
    "* [a-z]: Matches one of a, b, ..., z\n",
    "* [^abc]: Matches character NOT a, b or c\n",
    "* a|b: Mataches a or b, where a & b are strings\n",
    "* (): Scoping for operators\n",
    "* \\: Escape character for special characters e.g. \\t, \\n, \\b\n",
    "* \\b: Matches word boundary\n",
    "* \\d: Matches any digit [0-9]\n",
    "* \\D: Matches any non-digit [^0-9]\n",
    "* \\s: Matches any whitespace [\\t\\n\\r\\f\\v]\n",
    "* \\S: Matches any non-whitespace [^\\t\\n\\r\\f\\v]\n",
    "* \\w: Matches any alphanumeric [a-zA-Z0-9_]\n",
    "* \\W: Matches any non-alphanumeric [^a-zA-Z0-9_]\n",
    "* *: Matches 0 or more times\n",
    "* +: Matches 1 or more times\n",
    "* ?: Matches 0 or 1\n",
    "* {n}: Matches exactly n repititions, n>=0\n",
    "* {n,}: Matches at least n repititions, n>=0\n",
    "* {,n}: Matches at most n repititions, n>=0\n",
    "* {n,m}: Matches at least n & at most m repititions, n>=0, m>=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "s = 'The quick brown fox jumps over the lazy dog & = ##'\n",
    "\n",
    "# Match all alphanumeric\n",
    "print ([w for w in s.split(' ') if re.search('\\w', w)])\n",
    "\n",
    "dates = ['1/11/2020', '11/1/2020', 'Jan-1-2020', '1/11/20']\n",
    "print ([date for date in dates if re.findall(r'\\w{1,3}[/-]\\d{1,2}[/-]\\d{2,4}', date)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_sentences = [\"Monday: The doctor's appointment is at 2:45pm.\", \n",
    "                  \"Tuesday: The dentist's appointment is at 11:30 am.\",\n",
    "                  \"Wednesday: At 7:00pm, there is a basketball game!\",\n",
    "                  \"Thursday: Be back home by 11:15 pm at the latest.\",\n",
    "                  \"Friday: Take the train at 08:10 am, arrive at 09:00am.\"]\n",
    "\n",
    "df = pd.DataFrame(time_sentences, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find the number of characters for each string in df['text']\n",
    "df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find the number of tokens for each string in df['text']\n",
    "df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find which entries contain the word 'appointment'\n",
    "df['text'].str.contains('appointment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find how many times a digit occurs in each string\n",
    "df['text'].str.count(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find all occurances of the digits\n",
    "df['text'].str.findall(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# group and find the hours and minutes\n",
    "df['text'].str.findall(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace weekdays with '???'\n",
    "df['text'].str.replace(r'\\w+day\\b', '???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace weekdays with 3 letter abbrevations\n",
    "df['text'].str.replace(r'(\\w+day\\b)', lambda x: x.groups()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create new columns from first match of extracted groups\n",
    "df['text'].str.extract(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# extract the entire time, the hours, the minutes, and the period\n",
    "df['text'].str.extractall(r'((\\d?\\d):(\\d\\d) ?([ap]m))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# extract the entire time, the hours, the minutes, and the period with group names\n",
    "df['text'].str.extractall(r'(?P<time>(?P<hour>\\d?\\d):(?P<minute>\\d\\d) ?(?P<period>[ap]m))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "git config --global user.name \"<username>\"\n",
    "git config --global user.email \"<user email>\"\n",
    "git config --list\n",
    "git config --help\n",
    "\n",
    "git init\n",
    "git status\n",
    "\n",
    "git add -A\n",
    "git reset\n",
    "git commit -m \"<comment>\"\n",
    "git log\n",
    "\n",
    "git clone <remote repo> <destination>\n",
    "git remote -v\n",
    "git diff\n",
    "\n",
    "git pull origin master\n",
    "git push origin master\n",
    "\n",
    "# create branch\n",
    "git branch <branch name>\n",
    "\n",
    "# list branches & get branch status\n",
    "git branch \n",
    "git branch -a\n",
    "\n",
    "#switch to branch\n",
    "git checkout <branch name>\n",
    "\n",
    "#make changes to code, stage & commit to branch\n",
    "git add -a\n",
    "git commit -m <>\n",
    "\n",
    "#push changes from branch to remote repository\n",
    " git push -u origin <branch name>\n",
    "\n",
    "#merge changes with master\n",
    "git checkout master\n",
    "git pull origin master \n",
    "git branch --merged\n",
    "git merge hangman-lap\n",
    "git push origin master\n",
    "\n",
    "git reset --hard HEAD^  # Roll back last commit\n",
    "git reset --hard HEAD~2  # Roll back last 2 commits\n",
    "\n",
    "git branch -d <branch name> #delete local branch\n",
    "git push origin --delete <branch name> #delete remote branch\n",
    "\n",
    "# remove directories from git\n",
    "\n",
    "git rm -rf <dir name separated by space>  # not the -f flag is for forced removal.  Can do without it as well.\n",
    "git commit -m \"<comment>\"\n",
    "git push origin master\n",
    "\n",
    "-------------------\n",
    "git status\n",
    "\n",
    "git add *\n",
    "\n",
    "git commit -m \"Comments\"\n",
    "\n",
    "git push origin master\n",
    "\n",
    "git rm -r --cached djangoproject/\n",
    "\n",
    "git commit -m \"Comments\"\n",
    "\n",
    "git push origin master\n",
    "\n",
    "git pull\n",
    "\n",
    "git log\n",
    "\n",
    "git reset --hard <commit>  # get commit from git log\n",
    "git reset --hard origin/master\n",
    "\n",
    "git reflog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Virtual Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "virtualenv <env_name>  # will create virtual env env_name in directory env_name\n",
    "\n",
    "virtualenv . # will create virtual env in current dir\n",
    "\n",
    "scripts/activate\n",
    "\n",
    "scripts/deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Write\n",
    "# with open ('data/test.pkl', 'wb') as pklfile:\n",
    "#     pickle.dump(df, pklfile)\n",
    "\n",
    "# Read\n",
    "with open ('data/test.pkl', 'rb') as pklfile:\n",
    "    df = pickle.load(pklfile)\n",
    "    \n",
    "with open ('data/test1.pkl', 'rb') as pklfile:\n",
    "    df1 = pickle.load(pklfile)\n",
    "    \n",
    "with open ('data/test2.pkl', 'rb') as pklfile:\n",
    "    df2 = pickle.load(pklfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3 = df.copy()\n",
    "df3['winner'] = np.where(df['winner_enc']==df['fighter1_enc'], 1, 2)\n",
    "df3.drop(columns=['fighter1_enc', 'fighter2_enc', 'winner_enc'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Accessing index in for loops\n",
    "\n",
    "l = range(10,30,2)\n",
    "for idx, val in enumerate(l):\n",
    "    print (idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Aggregating elements across 2 iterables\n",
    "\n",
    "l1 = [1,2,3,4,5,6,7]\n",
    "l2 = ['violet', 'indigo', 'blue', 'green', 'yellow', 'orange', 'red']\n",
    "\n",
    "for val in zip (l1, l2):\n",
    "    print (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Progress bar\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "data = pd.read_csv('data/data.csv')\n",
    "ages = data['Age']\n",
    "dev_salaries = data['All_Devs']\n",
    "py_salaries = data['Python']\n",
    "js_salaries = data['JavaScript']\n",
    "\n",
    "plt.plot(ages, py_salaries, label='Python')\n",
    "plt.plot(ages, js_salaries, label='JavaScript')\n",
    "\n",
    "plt.plot(ages, dev_salaries, color='#444444',\n",
    "         linestyle='--', label='All Devs')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Median Salary (USD) by Age')\n",
    "plt.xlabel('Ages')\n",
    "plt.ylabel('Median Salary (USD)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "data = pd.read_csv('data/data.csv')\n",
    "ages = data['Age']\n",
    "dev_salaries = data['All_Devs']\n",
    "py_salaries = data['Python']\n",
    "js_salaries = data['JavaScript']\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax1.plot(ages, dev_salaries, color='#444444',\n",
    "         linestyle='--', label='All Devs')\n",
    "ax1.legend()\n",
    "ax1.set_title('Median Salary (USD) by Age')\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.set_ylabel('Median Salary (USD)')\n",
    "\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax2.plot(ages, py_salaries, label='Python')\n",
    "ax2.plot(ages, js_salaries, label='JavaScript')\n",
    "ax2.legend()\n",
    "ax2.set_title('Median Salary (USD) by Age')\n",
    "ax2.set_xlabel('Age')\n",
    "ax2.set_ylabel('Median Salary (USD)')\n",
    "\n",
    "ax3 = plt.subplot(2,2,3)\n",
    "ax3.plot(ages, py_salaries, label='Python')\n",
    "ax3.plot(ages, js_salaries, label='JavaScript')\n",
    "ax3.legend()\n",
    "ax3.set_title('Median Salary (USD) by Age')\n",
    "ax3.set_xlabel('Age')\n",
    "ax3.set_ylabel('Median Salary (USD)')\n",
    "\n",
    "ax4 = plt.subplot(2,2,4)\n",
    "ax4.plot(ages, dev_salaries, label='ALLDEVS')\n",
    "ax5 = ax4.twinx()\n",
    "ax5.plot(ages, js_salaries, label='JavaScript')\n",
    "ax4.legend()\n",
    "ax5.legend()\n",
    "ax4.set_xlabel('Age')\n",
    "ax4.set_ylabel('Python Salary (USD)')\n",
    "ax5.set_ylabel('Javascript Salary (USD)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "data = pd.read_csv('data/data.csv')\n",
    "ages = data['Age']\n",
    "dev_salaries = data['All_Devs']\n",
    "py_salaries = data['Python']\n",
    "js_salaries = data['JavaScript']\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(15,7))\n",
    "\n",
    "# fig1, ax1 = plt.subplots()\n",
    "# fig2, ax2 = plt.subplots()\n",
    "\n",
    "ax1.plot(ages, dev_salaries, color='#444444',\n",
    "         linestyle='--', label='All Devs')\n",
    "\n",
    "ax2.plot(ages, py_salaries, label='Python')\n",
    "ax2.plot(ages, js_salaries, label='JavaScript')\n",
    "\n",
    "ax3.plot(ages, dev_salaries, label='ALLDEVS')\n",
    "ax4 = ax3.twinx()                             # Twinx\n",
    "ax4.plot(ages, py_salaries, label='PYTHON')\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_title('Median Salary (USD) by Age')\n",
    "ax1.set_ylabel('Median Salary (USD)')\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Ages')\n",
    "ax2.set_ylabel('Median Salary (USD)')\n",
    "\n",
    "ax3.legend()\n",
    "ax3.set_title('MEDIAN SALARY (USD) by AGE')\n",
    "ax3.set_ylabel('MEDIAN SALARY (USD)')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig1.savefig('data/fig1.png')\n",
    "fig2.savefig('data/fig2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Resize & Center images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Standardize images\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function to resize image\n",
    "def resize_image(img, size): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    img.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square white background image\n",
    "    newimg = Image.new(\"RGB\", size, (255, 255, 255))\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    if np.array(img).shape[2] == 4:\n",
    "        # If the source is in RGBA format, use a mask to eliminate the transparency\n",
    "        newimg.paste(img, (int((size[0] - img.size[0]) / 2), int((size[1] - img.size[1]) / 2)), mask=img.split()[3])\n",
    "    else:\n",
    "        newimg.paste(img, (int((size[0] - img.size[0]) / 2), int((size[1] - img.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return newimg\n",
    "\n",
    "\n",
    "# Create resized copies of all of the source images\n",
    "size = (128,128)\n",
    "\n",
    "indir = 'gear_images'\n",
    "outdir = 'resized_images'\n",
    "\n",
    "# Create the output folder if it doesn't already exist\n",
    "if os.path.exists(outdir):\n",
    "    shutil.rmtree(outdir)\n",
    "\n",
    "# Loop through each subfolder in the input dir\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for d in dirs:\n",
    "        print('processing folder ' + d)\n",
    "        # Create a matching subfolder in the output dir\n",
    "        saveFolder = os.path.join(outdir,d)\n",
    "        if not os.path.exists(saveFolder):\n",
    "            os.makedirs(saveFolder)\n",
    "        # Loop through the files in the subfolder\n",
    "        files = os.listdir(os.path.join(root,d))\n",
    "        for f in files:\n",
    "            # Open the file\n",
    "            imgFile = os.path.join(root,d, f)\n",
    "            print(\"reading \" + imgFile)\n",
    "            img = Image.open(imgFile)\n",
    "            # Create a resized version and save it\n",
    "            proc_img = resize_image(img, size)\n",
    "            saveAs = os.path.join(saveFolder, 'resized_' + f)\n",
    "            print(\"writing \" + saveAs)\n",
    "            proc_img.save(saveAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Resize & Center images - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Helper function to resize image\n",
    "def resize_image(img, size): \n",
    "    from PIL import Image, ImageOps \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    img.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square white background image\n",
    "    newimg = Image.new(\"RGB\", size, (255, 255, 255))\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    if np.array(img).shape[2] == 4:\n",
    "        # If the source is in RGBA format, use a mask to eliminate the transparency\n",
    "        newimg.paste(img, (int((size[0] - img.size[0]) / 2), int((size[1] - img.size[1]) / 2)), mask=img.split()[3])\n",
    "    else:\n",
    "        newimg.paste(img, (int((size[0] - img.size[0]) / 2), int((size[1] - img.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return newimg\n",
    "\n",
    "\n",
    "def standardize_image(image):\n",
    "    size = (128,128)\n",
    "    img = Image.open(image)\n",
    "    std_img = np.array(resize_image(img, size))\n",
    "    std_img = std_img.astype('float32')\n",
    "    std_img /= 255\n",
    "    std_img = std_img.reshape(1,128,128,3)\n",
    "    return (std_img)\n",
    "\n",
    "#################################################################\n",
    "test_imgs = ['test_img1.jpg', 'test_img2.jpg', 'test_img3.jpg']\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "i=0\n",
    "no_imgs = len(test_imgs)\n",
    "\n",
    "for test_img in test_imgs:\n",
    "    std_img = standardize_image(test_img)\n",
    "    pred = model.predict_classes(std_img)\n",
    "    ax = plt.subplot(1,no_imgs,i+1)\n",
    "    ax.imshow(std_img[0])\n",
    "    ax.set_title(f'{pred}=>{get_key(pred)}')\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Image retrieval\n",
    "# -  Search images in __[Google Images](https://images.google.com)__\n",
    "# -  Hit __CtrlShiftJ__ in Windows/Linux or __CmdOptJ__ in Mac\n",
    "# -  Type the following javascript code into the window & hit enter to download image URL file\n",
    "\n",
    "urls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);\n",
    "window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n')));\n",
    "\n",
    "import requests\n",
    "url_file = 'data/url.txt'\n",
    "i = 0\n",
    "\n",
    "with open (url_file, 'rb') as f:\n",
    "    for line in f:\n",
    "        url = line.rstrip().decode(\"utf-8\")\n",
    "        filename = url.split('/')[-1]\n",
    "        r = requests.get(url)\n",
    "        with open (f'data/images/img_{i}.png', 'wb') as img:\n",
    "            img.write(r.content)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url_file = 'data/images/MartialArts/muaythai.txt'\n",
    "label = url_file.split('/')[-1].split('.')[0]\n",
    "\n",
    "i = 0\n",
    "\n",
    "with open (url_file, 'rb') as f:\n",
    "    for line in f:\n",
    "        url = line.rstrip().decode(\"utf-8\")\n",
    "        filename = url.split('/')[-1]\n",
    "        r = requests.get(url)\n",
    "        with open (f'data/images/MartialArts/{label}/img_{i}.png', 'wb') as img:\n",
    "            img.write(r.content)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read & Display images\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img_arr = cv2.imread('data/images/img_1.png')\n",
    "print (f'Image size = {img_arr.shape}')\n",
    "plt.imshow(img_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert to grayscale\n",
    "\n",
    "img_arr_gs = cv2.imread('data/images/img_1.png', cv2.IMREAD_GRAYSCALE)\n",
    "print (f'Grayscale image. Shape = {img_arr_gs.shape}')\n",
    "plt.imshow (img_arr_gs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Image resize\n",
    "\n",
    "size = (224, 224)\n",
    "\n",
    "img_arr_resized = cv2.resize(img_arr, size)\n",
    "print (f'Resized original image. Shape = {img_arr_resized.shape}')\n",
    "plt.imshow(img_arr_resized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Overlay image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gorilla = cv2.imread('data/images/gorilla.jpg')\n",
    "lion = cv2.imread('data/images/lion.jpg')\n",
    "lion_resize = cv2.resize(lion, (224,224))\n",
    "lion.shape, lion_resize.shape, gorilla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_offset=1000\n",
    "y_offset=50\n",
    "gorilla[y_offset:y_offset+lion.shape[0], x_offset:x_offset+lion.shape[1]] = lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(gorilla)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## [Pillow](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_1_python_images.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_path = 'data/images/MartialArts/train'\n",
    "valid_path = 'data/images/MartialArts/valid'\n",
    "test_path = 'data/images/MartialArts/test'\n",
    "\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(224,224), classes=['bjj', 'muaythai', 'boxing'], batch_size=2)\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path, target_size=(224,224), classes=['bjj', 'muaythai', 'boxing'], batch_size=2)\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, target_size=(224,224), classes=['bjj', 'muaythai', 'boxing'], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "X_train_preproc = preprocess_input(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd              \n",
    "import category_encoders as ce   \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Guitar Make': ['Fender', 'Gibson', 'Ibanez', 'Rickenbacker', 'Jackson'],\n",
    "        'Color': ['Red', 'Black', 'Blue', 'Black', 'Red']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Label\n",
    "le = LabelEncoder()\n",
    "encoded = le.fit_transform(df['Guitar Make'])\n",
    "encoded\n",
    "\n",
    "# Ordinal\n",
    "ce_ord = ce.OrdinalEncoder(cols = ['Guitar Make', 'Color'])\n",
    "ce_ord.fit_transform(df)\n",
    "\n",
    "# One-hot\n",
    "ce_onehot = ce.OneHotEncoder(cols = ['Guitar Make', 'Color'])\n",
    "ce_onehot.fit_transform(df)\n",
    "\n",
    "# Binary\n",
    "ce_binary = ce.BinaryEncoder(cols = ['Guitar Make', 'Color'])\n",
    "ce_binary.fit_transform(df)\n",
    "\n",
    "# BaseN\n",
    "ce_baseN = ce.BaseNEncoder(cols = ['Guitar Make', 'Color'])\n",
    "ce_baseN.fit_transform(df)\n",
    "\n",
    "# Hashing\n",
    "ce_hashing = ce.HashingEncoder(cols = ['Guitar Make', 'Color'])\n",
    "ce_hashing.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Splitting into Train & Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Scaling, Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scale(scaler, data):\n",
    "  from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "  \n",
    "  if (scaler == 'standard'):\n",
    "    std_scaler = StandardScaler()\n",
    "    return (std_scaler.fit_transform(data))\n",
    "  elif (scaler == 'minmax'):\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    return (minmax_scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = scale('standard', df)\n",
    "# X = scale('minmax', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pca (data):\n",
    "  from sklearn.decomposition import PCA\n",
    "#   pca = PCA(n_components=10)\n",
    "  pca = PCA(.95)   # retain 95% of variance\n",
    "  principalComponents_train = pca.fit_transform(data)\n",
    "  pac_vars_train = [print (f'{pac_var*100:.2f}%\\t', end=\"\") for pac_var in pca.explained_variance_ratio_]\n",
    "  print ()\n",
    "  print (f'{np.sum(pca.explained_variance_ratio_)*100:.2f}%')\n",
    "  return (principalComponents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_scaled = scale('standard', df3.iloc[:,:-1])\n",
    "pc = pca(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for c in list(string.ascii_lowercase):\n",
    "    fighters_url = f'http://ufcstats.com/statistics/fighters?char={c}&page=all'\n",
    "    \n",
    "    page = requests.get(fighters_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    fighters_data = soup.find_all('tr')\n",
    "\n",
    "    for fighter in fighters_data[2:]:\n",
    "        fighter_data = fighter.find_all('td')\n",
    "        first_names.append(fighter_data[0].get_text(strip=True))\n",
    "        last_names.append(fighter_data[1].get_text(strip=True))\n",
    "        nick_names.append(fighter_data[2].get_text(strip=True))\n",
    "        heights.append(fighter_data[3].get_text(strip=True))\n",
    "        weights.append(fighter_data[4].get_text(strip=True))\n",
    "        reaches.append(fighter_data[5].get_text(strip=True))\n",
    "        stances.append(fighter_data[6].get_text(strip=True))\n",
    "        wins.append(fighter_data[7].get_text(strip=True))\n",
    "        losses.append(fighter_data[8].get_text(strip=True))\n",
    "        draws.append(fighter_data[9].get_text(strip=True))\n",
    "        fighter_urls.append(fighter_data[0].find('a')['href'])\n",
    "        \n",
    "fighter_data = pd.DataFrame(\n",
    "    {\n",
    "        'first_name' : first_names,\n",
    "        'last_name' : last_names,\n",
    "        'nick_name' : nick_names,\n",
    "        'height' : heights,\n",
    "        'weight' : weights,\n",
    "        'reach' : reaches,\n",
    "        'stance' : stances,\n",
    "        'wins' : wins,\n",
    "        'losses' : losses,\n",
    "        'draws' : draws,\n",
    "        'fighter_url' : fighter_urls,\n",
    "    }\n",
    ")\n",
    "\n",
    "fighter_data.to_csv('data/fighter_data_base.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read Excel\n",
    "\n",
    "df_fight_card = pd.read_excel('../data/fight_card.xlsx', sheet_name='Sheet2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "\n",
    "df_fight_card = pd.DataFrame(\n",
    "    {\n",
    "        'weight_class' : [\"Women's Strawweight\", \"Welterweight\", \"Flyweight\", \"Women's Flyweight\", \"Middleweight\", \n",
    "                          \"Lightweight\"],\n",
    "        'fighter1' : [\"Jessica Andrade\", \"Li Jingliang\", \"Kai Kara-France\", \"Wu Yanan\", \"Robert Whittaker\",\n",
    "                      \"Khabib Nurmagomedov\",],\n",
    "        'fighter2' : [\"Weili Zhang\", \"Elizeu Zaleski dos Santos\", \"Mark De La Rosa\", \"Mizuki Inoue\", \"Israel Adesanya\",\n",
    "                      \"Dustin Poirier\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data joins, vLookup\n",
    "\n",
    "df1 = df1.merge(df2, left_on=['fighter1'], right_on=['full_name'], how='left').\\\n",
    "drop(columns=['first_name', 'last_name', 'nick_name', 'fighter_url'])\n",
    "\n",
    "df1.rename(columns={'height':'fighter1_height', 'weight':'fighter1_weight', 'reach':'fighter1_reach',\\\n",
    "                                 'stance':'fighter1_stance','wins':'fighter1_wins', 'losses':'fighter1_losses',\\\n",
    "                                 'draws':'fighter1_draws', 'SLpM':'fighter1_SLpM', 'Str_Acc':'fighter1_Str_Acc',\\\n",
    "                                 'SApM':'fighter1_SApM', 'Str_Dep':'fighter1_Str_Dep','TD_Avg':'fighter1_TD_Avg',\\\n",
    "                                 'TD_Acc':'fighter1_TD_Acc', 'TD_Def':'fighter1_TD_Def',\\\n",
    "                                 'Sub_Avg':'fighter1_Sub_Avg','full_name':'fighter1_full_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Copy DataFrame\n",
    "\n",
    "df3 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Column value based on conditional\n",
    "\n",
    "df['winner'] = np.where(df['winner_enc']==df['fighter1_enc'], 1, 2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "\n",
    "df.isna().any()\n",
    "\n",
    "cols_nan = df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Slice\n",
    "\n",
    "df.loc[:6,:]\n",
    "df.iloc[:6,:]\n",
    "df.col_0\n",
    "df['col_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Conditionals\n",
    "\n",
    "df1['winner_en'] = np.where(df1['winner']==df1['fighter1'], 0, 1)\n",
    "df1[df1['fighter1'].str.contains('Jessica Andrade')]\n",
    "df1[df1.fighter1 == 'Jessica Andrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert to numeric\n",
    "\n",
    "#One Series\n",
    "y_test_price = pd.to_numeric(y_test.loc[:,'price'], errors='coerce')\n",
    "\n",
    "#Entire dataframe\n",
    "X_train = X_train.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=3)\n",
    "X = np.array(df.iloc[:,:-1])\n",
    "y = np.array(df.iloc[:,-1])\n",
    "\n",
    "scores_rf = []\n",
    "scores_logistic = []\n",
    "scores_svm = []\n",
    "\n",
    "for train_index, test_index in folds.split(X, y):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], \\\n",
    "                                       y[train_index], y[test_index]\n",
    "    \n",
    "    scores_rf.append(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))\n",
    "#     scores_logistic.append(get_score(LogisticRegression(), X_train, X_test, y_train, y_test))  \n",
    "#     scores_svm.append(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.mean(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# X = np.array(df.iloc[:,:-1])\n",
    "X = scale('standard', df.iloc[:,:-1])\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "\n",
    "score_rf = cross_val_score(RandomForestClassifier(n_estimators=40),X, y, cv=3)\n",
    "np.mean(score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ML Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df3.iloc[:,:-1]\n",
    "y = df3.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = scale('standard', X_train)\n",
    "X_test = scale('standard', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree = tree.DecisionTreeClassifier(criterion='entropy', random_state=9, max_depth=11, max_features=13)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_score = tree.score(X_test, y_test)\n",
    "print (f'Decision Tree score on test data = {tree_score*100:.2f}%')\n",
    "\n",
    "y_pred_tree = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Random Forrest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=42, max_depth=10, max_features=20)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print (f'Random Forrest score on test data = {rf_score*100:.2f}%')\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_params = {\n",
    "                 'boosting_type' : 'dart', 'objective' : 'binary','learning_rate' : 0.1,\n",
    "                  'n_estimators' : 100,  'max_bin' : 100, 'n_jobs' : 2, 'num_leaves' : 50,\n",
    "                 }\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_score = lgbm.score(X_test, y_test)\n",
    "print (f'LightGBM score on test data = {lgbm_score*100:.2f}%')\n",
    "\n",
    "y_pred_lgbm = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open ('data/TheOldManAndTheSea.txt', 'r') as f:\n",
    "    book_txt = f.read()\n",
    "\n",
    "book_txt = book_txt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "\n",
    "words = nltk.word_tokenize(book_txt)\n",
    "len (words), len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "\n",
    "sentences = nltk.sent_tokenize(book_txt)\n",
    "len(sentences), len(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences[-1], words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Ferquency distribution\n",
    "dist = FreqDist(words)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Frequent words, where a word lenght is greater than or equal to 3 characters and it occurs more than 5 times\n",
    "freq_words = [w for w in dist.keys() if len(w) >= 3 and dist[w] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "stemmed_words = [porter.stem(w) for w in freq_words]\n",
    "# stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "WNLemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = [WNLemma.lemmatize(w) for w in freq_words]\n",
    "# lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "? nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Part of Speech (POS) tagging\n",
    "\n",
    "pos_tag = nltk.pos_tag(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# NLTK classifier\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('data/Amazon_Unlocked_Mobile.csv')\n",
    "\n",
    "# Sample the data to speed up computation\n",
    "# Comment out this line to match with lecture\n",
    "df = df.sample(frac=0.1, random_state=10)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['Rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (rated positively)\n",
    "# Encode 1s and 2s as 0 (rated poorly)\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Most ratings are positive (skewed data set)\n",
    "df['Positively Rated'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n",
    "                                                    df['Positively Rated'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'X_train first entry:{X_train.iloc[0]}')\n",
    "print(f'\\n\\nX_train shape: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### CountVectorizer (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# Can specify min_df to reduce number of features\n",
    "# In the case below, remove words from vocab which appear in less than 5 documents\n",
    "# vect = CountVectorizer(min_df=5).fit(X_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vect.get_feature_names()[::2000]  # Look at every 2,000th feature in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "# Gives BOW representation of X_train. X_test\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "X_train_vectorized.shape, X_test_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(X_test_vectorized)\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Term Frequency Inverse Document Frequency (Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "# Remove words from vocab which appear in less than 5 documents\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())  # The num of features reduced to 5442 from 19601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))  # Approx same accuracy with fewer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These reviews are treated the same by our current model\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions)) # Slightly higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These reviews are now correctly identified\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text & store in dictionary\n",
    "\n",
    "books = ['TheOldManAndTheSea', 'TheAdventuresofSherlock Holmes', 'MobyDick']\n",
    "data_txt = dict()\n",
    "\n",
    "for book in books:\n",
    "    with open (f'data/{book}.txt', 'rb') as f:\n",
    "        data_txt[book] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <td>b'\\xef\\xbb\\xbf\"What I want you to do,\" said Mr. George Wright, as he leaned towards the\\r\\nold sailor, \"is to be an uncle to me.\"\\r\\n\\r\\n\"Aye, aye,\" said the mystified Mr. Kemp, pausing with a mug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <td>b'I. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard him\\r\\nmention her under any other name. In his eyes she eclipses and\\r\\npredominat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobyDick</th>\n",
       "      <td>b'\\xef\\xbb\\xbf\\r\\nCHAPTER 1. Loomings.\\r\\n\\r\\nCall me Ishmael. Some years ago\\xe2\\x80\\x94never mind how long precisely\\xe2\\x80\\x94having\\r\\nlittle or no money in my purse, and nothing particular t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                   Text\n",
       "TheOldManAndTheSea              b'\\xef\\xbb\\xbf\"What I want you to do,\" said Mr. George Wright, as he leaned towards the\\r\\nold sailor, \"is to be an uncle to me.\"\\r\\n\\r\\n\"Aye, aye,\" said the mystified Mr. Kemp, pausing with a mug...\n",
       "TheAdventuresofSherlock Holmes  b'I. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard him\\r\\nmention her under any other name. In his eyes she eclipses and\\r\\npredominat...\n",
       "MobyDick                        b'\\xef\\xbb\\xbf\\r\\nCHAPTER 1. Loomings.\\r\\n\\r\\nCall me Ishmael. Some years ago\\xe2\\x80\\x94never mind how long precisely\\xe2\\x80\\x94having\\r\\nlittle or no money in my purse, and nothing particular t..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store in DataFrame - book name as key & book text as val\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_colwidth', 200)\n",
    "\n",
    "# Store each dict val as a list (required for creating DataFrame)        \n",
    "data_txt = {key:[val] for (key, val) in data_txt.items()}\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_txt).transpose()\n",
    "data_df.columns = ['Text']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <td>﻿what i want you to do said mr george wright as he leaned towards the  old sailor is to be an uncle to me    aye aye said the mystified mr kemp pausing with a mug of beer  midway to his lips    a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <td>i a scandal in bohemia      i    to sherlock holmes she is always the woman i have seldom heard him  mention her under any other name in his eyes she eclipses and  predominates the whole of her se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobyDick</th>\n",
       "      <td>﻿  chapter  loomings    call me ishmael some years ago—never mind how long precisely—having  little or no money in my purse and nothing particular to interest me  on shore i thought i would sail a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                   Text\n",
       "TheOldManAndTheSea              ﻿what i want you to do said mr george wright as he leaned towards the  old sailor is to be an uncle to me    aye aye said the mystified mr kemp pausing with a mug of beer  midway to his lips    a ...\n",
       "TheAdventuresofSherlock Holmes  i a scandal in bohemia      i    to sherlock holmes she is always the woman i have seldom heard him  mention her under any other name in his eyes she eclipses and  predominates the whole of her se...\n",
       "MobyDick                        ﻿  chapter  loomings    call me ishmael some years ago—never mind how long precisely—having  little or no money in my purse and nothing particular to interest me  on shore i thought i would sail a..."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic text cleaning\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(txt):\n",
    "    if (type(txt) == bytes):\n",
    "        txt = txt.decode(\"utf-8\")   # Decode bytes to string\n",
    "    txt = txt.lower()   # Make lowercase\n",
    "    txt = ''.join(c for c in txt if c not in set(string.punctuation))   # Remove punctuation\n",
    "    txt = re.sub('[\\r\\n]', ' ', txt)   # Remove carriage return & line feed characters\n",
    "    txt = re.sub('\\d*', '', txt)  # Remove numbers\n",
    "    return (txt)\n",
    "        \n",
    "clean = lambda x: clean_text(x)\n",
    "\n",
    "data_df_clean = pd.DataFrame(data_df.Text.apply(clean))\n",
    "data_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <td>want you said georg wright the old uncl kemp with hi rich young man voic ani next from new zealand who go all money come littl wa youv onli got say ive anoth after and she make that went never tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <td>scandal bohemia sherlock holm she alway the woman have seldom heard him her under ani other name hi eye and whole wa not that felt emot love for iren adler all one particularli were cold but mind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobyDick</th>\n",
       "      <td>chapter call ishmael some year mind how long littl money purs and noth particular interest shore thought would sail about see the wateri part world way have drive off whenev find myself grow grim ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                   Text\n",
       "TheOldManAndTheSea              want you said georg wright the old uncl kemp with hi rich young man voic ani next from new zealand who go all money come littl wa youv onli got say ive anoth after and she make that went never tha...\n",
       "TheAdventuresofSherlock Holmes  scandal bohemia sherlock holm she alway the woman have seldom heard him her under ani other name hi eye and whole wa not that felt emot love for iren adler all one particularli were cold but mind ...\n",
       "MobyDick                        chapter call ishmael some year mind how long littl money purs and noth particular interest shore thought would sail about see the wateri part world way have drive off whenev find myself grow grim ..."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose words of certain length & frequency and apply stemming\n",
    "\n",
    "import nltk\n",
    "\n",
    "def stemming(txt):\n",
    "    tokenized_words = nltk.word_tokenize(txt)       # Tokenize the text - split into words\n",
    "    freq_dist = nltk.FreqDist(tokenized_words)      # Get Frequency Distibution of words\n",
    "    freq_tokenized_words = [w for (w, f) in freq_dist.items() if len(w) >= 3 and f >= 5]  # Remove words less than 3 chars & occurring less than 5 times \n",
    "    porter = nltk.PorterStemmer()         # Apply stemming to tokenized list\n",
    "    stemmed_words = [porter.stem(w) for w in freq_tokenized_words]\n",
    "    txt = ' '.join(w for w in stemmed_words)   # Join stemmed word list into string\n",
    "    return (txt)\n",
    "\n",
    "stem = lambda x: stemming(x)\n",
    "\n",
    "data_df_clean_stem_freq = pd.DataFrame(data_df_clean.Text.apply(stem))\n",
    "data_df_clean_stem_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle original and cleaned corpora\n",
    "\n",
    "import pickle\n",
    "\n",
    "# with open ('data/data_df.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_df, f)\n",
    "# with open ('data/data_df_clean.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_df_clean, f)\n",
    "# with open ('data/data_df_clean_stem_freq.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_df_clean_stem_freq, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Cloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DTM, TDM & prep for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['she', 'he', 'him', 'they', 'and'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abl</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abound</th>\n",
       "      <th>abov</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absenc</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accid</th>\n",
       "      <th>...</th>\n",
       "      <th>yon</th>\n",
       "      <th>yonder</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>yourselv</th>\n",
       "      <th>youth</th>\n",
       "      <th>youv</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobyDick</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3298 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                abandon  abl  aboard  abound  abov  abroad  \\\n",
       "TheOldManAndTheSea                    0    0       0       0     0       0   \n",
       "TheAdventuresofSherlock Holmes        0    1       0       0     1       0   \n",
       "MobyDick                              1    1       1       1     1       1   \n",
       "\n",
       "                                absenc  absent  absolut  accid  ...  yon  \\\n",
       "TheOldManAndTheSea                   0       0        0      0  ...    0   \n",
       "TheAdventuresofSherlock Holmes       0       0        2      1  ...    0   \n",
       "MobyDick                             1       1        0      2  ...    1   \n",
       "\n",
       "                                yonder  york  young  yourselv  youth  youv  \\\n",
       "TheOldManAndTheSea                   0     0      1         0      0     1   \n",
       "TheAdventuresofSherlock Holmes       0     0      1         0      1     0   \n",
       "MobyDick                             1     1      1         1      1     0   \n",
       "\n",
       "                                zealand  zodiac  zone  \n",
       "TheOldManAndTheSea                    1       0     0  \n",
       "TheAdventuresofSherlock Holmes        0       0     0  \n",
       "MobyDick                              1       1     1  \n",
       "\n",
       "[3 rows x 3298 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DTM (document-term matrix)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_df_clean_stem_freq.Text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_df_clean_stem_freq.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the DTM & CountVectorizer object\n",
    "\n",
    "# with open ('data/data_dtm.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_dtm, f)\n",
    "# with open ('data/cv.pkl', 'wb') as f:\n",
    "#     pickle.dump(cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <th>MobyDick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abl</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aboard</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abound</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abov</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TheOldManAndTheSea  TheAdventuresofSherlock Holmes  MobyDick\n",
       "abandon                   0                               0         1\n",
       "abl                       0                               1         1\n",
       "aboard                    0                               0         1\n",
       "abound                    0                               0         1\n",
       "abov                      0                               1         1"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a TDM (term-document matrix), which is one of the required inputs\n",
    "\n",
    "data_tdm = data_dtm.transpose()\n",
    "data_tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for gensim\n",
    "\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# Convert TDM into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(data_tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "# Gensim also requires a dictionary of the all terms and their respective location in the TDM\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Latent Direchlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using earlier preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"ill\" + 0.001*\"wont\" + 0.001*\"zealand\" + 0.001*\"vigor\" + 0.001*\"steed\" + 0.001*\"hill\" + 0.001*\"would\" + 0.001*\"anoth\" + 0.001*\"pound\" + 0.001*\"shook\"'),\n",
       " (1,\n",
       "  '0.001*\"continu\" + 0.001*\"spout\" + 0.001*\"harpoon\" + 0.001*\"live\" + 0.001*\"sound\" + 0.001*\"breath\" + 0.001*\"use\" + 0.001*\"murder\" + 0.001*\"carri\" + 0.001*\"appear\"'),\n",
       " (2,\n",
       "  '0.000*\"continu\" + 0.000*\"present\" + 0.000*\"hand\" + 0.000*\"carri\" + 0.000*\"consider\" + 0.000*\"watch\" + 0.000*\"place\" + 0.000*\"live\" + 0.000*\"call\" + 0.000*\"show\"'),\n",
       " (3,\n",
       "  '0.000*\"watch\" + 0.000*\"use\" + 0.000*\"bow\" + 0.000*\"place\" + 0.000*\"point\" + 0.000*\"appear\" + 0.000*\"work\" + 0.000*\"remain\" + 0.000*\"glanc\" + 0.000*\"look\"'),\n",
       " (4,\n",
       "  '0.005*\"come\" + 0.004*\"ill\" + 0.004*\"wont\" + 0.004*\"zealand\" + 0.003*\"would\" + 0.003*\"anoth\" + 0.003*\"want\" + 0.003*\"take\" + 0.003*\"look\" + 0.003*\"say\"'),\n",
       " (5,\n",
       "  '0.000*\"continu\" + 0.000*\"call\" + 0.000*\"appear\" + 0.000*\"live\" + 0.000*\"use\" + 0.000*\"look\" + 0.000*\"reason\" + 0.000*\"tri\" + 0.000*\"consider\" + 0.000*\"present\"'),\n",
       " (6,\n",
       "  '0.002*\"close\" + 0.002*\"reason\" + 0.002*\"live\" + 0.002*\"point\" + 0.002*\"remark\" + 0.002*\"appear\" + 0.002*\"look\" + 0.002*\"examin\" + 0.001*\"use\" + 0.001*\"seem\"'),\n",
       " (7,\n",
       "  '0.000*\"place\" + 0.000*\"float\" + 0.000*\"direct\" + 0.000*\"paint\" + 0.000*\"sound\" + 0.000*\"contain\" + 0.000*\"feel\" + 0.000*\"use\" + 0.000*\"coast\" + 0.000*\"harpoon\"'),\n",
       " (8,\n",
       "  '0.000*\"die\" + 0.000*\"point\" + 0.000*\"use\" + 0.000*\"murder\" + 0.000*\"enter\" + 0.000*\"live\" + 0.000*\"gener\" + 0.000*\"breath\" + 0.000*\"compar\" + 0.000*\"remain\"'),\n",
       " (9,\n",
       "  '0.000*\"name\" + 0.000*\"carri\" + 0.000*\"reason\" + 0.000*\"appear\" + 0.000*\"use\" + 0.000*\"continu\" + 0.000*\"say\" + 0.000*\"breath\" + 0.000*\"seem\" + 0.000*\"live\"')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of topics and the number of passes for LDA\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to Nouns & Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <td>i george wright old sailor uncle aye mr kemp mug beer midway lips rich uncle young man voice keen ears next bar useless knowledge uncle new zealand money wheres mr kemp little excitement reply you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <td>scandal bohemia i holmes woman i other name eyes whole sex emotion akin irene adler emotions abhorrent cold precise i perfect reasoning machine world lover false position softer passions gibe snee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobyDick</th>\n",
       "      <td>﻿ chapter loomings years long precisely—having little money purse nothing particular interest shore i little watery part world way i spleen circulation i grim mouth damp drizzly november soul i co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                   Text\n",
       "TheOldManAndTheSea              i george wright old sailor uncle aye mr kemp mug beer midway lips rich uncle young man voice keen ears next bar useless knowledge uncle new zealand money wheres mr kemp little excitement reply you...\n",
       "TheAdventuresofSherlock Holmes  scandal bohemia i holmes woman i other name eyes whole sex emotion akin irene adler emotions abhorrent cold precise i perfect reasoning machine world lover false position softer passions gibe snee...\n",
       "MobyDick                        ﻿ chapter loomings years long precisely—having little money purse nothing particular interest shore i little watery part world way i spleen circulation i grim mouth damp drizzly november soul i co..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use nouns and adjectves from text which has been basic cleaned\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def get_nouns_adj(txt):\n",
    "    tokenized_words = nltk.word_tokenize(txt)\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized_words) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "\n",
    "nouns_adj = lambda x: get_nouns_adj(x)\n",
    "data_df_clean_NounsAdj = pd.DataFrame(data_df_clean.Text.apply(nouns_adj))\n",
    "data_df_clean_NounsAdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <td>georg wright old uncl kemp rich young man voic new money littl ive other itll home time pound week good best pocketbook even way last miss bradshaw few shop mr tomorrow dont bella hill head half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <td>scandal bohemia holm woman other name eye whole emot iren adler cold perfect reason machin world lover posit sneer thing veil men reason such own delic doubt result strong natur late memori littl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobyDick</th>\n",
       "      <td>chapter year long littl money purs noth particular interest shore wateri part world way grim mouth damp soul coffin rear funer such upper hand strong principl street peopl high time pistol ball sw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                   Text\n",
       "TheOldManAndTheSea                   georg wright old uncl kemp rich young man voic new money littl ive other itll home time pound week good best pocketbook even way last miss bradshaw few shop mr tomorrow dont bella hill head half\n",
       "TheAdventuresofSherlock Holmes  scandal bohemia holm woman other name eye whole emot iren adler cold perfect reason machin world lover posit sneer thing veil men reason such own delic doubt result strong natur late memori littl ...\n",
       "MobyDick                        chapter year long littl money purs noth particular interest shore wateri part world way grim mouth damp soul coffin rear funer such upper hand strong principl street peopl high time pistol ball sw..."
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose words of certain length & frequency and apply stemming on Nouns & Adjectives\n",
    "\n",
    "data_df_clean_NounsAdj_stem_freq = pd.DataFrame(data_df_clean_NounsAdj.Text.apply(stem))\n",
    "data_df_clean_NounsAdj_stem_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>abov</th>\n",
       "      <th>absenc</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accid</th>\n",
       "      <th>account</th>\n",
       "      <th>accurs</th>\n",
       "      <th>acquaint</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yojo</th>\n",
       "      <th>yonder</th>\n",
       "      <th>york</th>\n",
       "      <th>yourselv</th>\n",
       "      <th>youth</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zodiac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobyDick</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2485 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                abl  abov  absenc  absent  absolut  accid  \\\n",
       "TheOldManAndTheSea                0     0       0       0        0      0   \n",
       "TheAdventuresofSherlock Holmes    1     0       0       0        1      1   \n",
       "MobyDick                          1     1       1       1        0      2   \n",
       "\n",
       "                                account  accurs  acquaint  act  ...  year  \\\n",
       "TheOldManAndTheSea                    0       0         0    0  ...     0   \n",
       "TheAdventuresofSherlock Holmes        1       0         1    0  ...     2   \n",
       "MobyDick                              2       1         0    1  ...     2   \n",
       "\n",
       "                                yellow  yesterday  yojo  yonder  york  \\\n",
       "TheOldManAndTheSea                   0          0     0       0     0   \n",
       "TheAdventuresofSherlock Holmes       1          1     0       0     0   \n",
       "MobyDick                             1          1     1       1     1   \n",
       "\n",
       "                                yourselv  youth  zealand  zodiac  \n",
       "TheOldManAndTheSea                     0      0        0       0  \n",
       "TheAdventuresofSherlock Holmes         0      1        0       0  \n",
       "MobyDick                               1      1        1       1  \n",
       "\n",
       "[3 rows x 2485 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new DTM (document-term matrix)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv_na = CountVectorizer(stop_words=stop_words, max_df=.8)  # Ignore stop words &  words occurring in 80% of documents\n",
    "data_cv_na = cv_na.fit_transform(data_df_clean_NounsAdj_stem_freq.Text)\n",
    "data_dtm_na = pd.DataFrame(data_cv_na.toarray(), columns=cv_na.get_feature_names())\n",
    "data_dtm_na.index = data_df_clean_NounsAdj_stem_freq.index\n",
    "data_dtm_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TheOldManAndTheSea</th>\n",
       "      <th>TheAdventuresofSherlock Holmes</th>\n",
       "      <th>MobyDick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abl</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abov</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absenc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolut</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TheOldManAndTheSea  TheAdventuresofSherlock Holmes  MobyDick\n",
       "abl                       0                               1         1\n",
       "abov                      0                               0         1\n",
       "absenc                    0                               0         1\n",
       "absent                    0                               0         1\n",
       "absolut                   0                               1         0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a TDM (term-document matrix), which is one of the required inputs\n",
    "\n",
    "data_tdm_na = data_dtm_na.transpose()\n",
    "data_tdm_na.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for gensim\n",
    "\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# Convert TDM into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts_na = scipy.sparse.csr_matrix(data_tdm_na)\n",
    "corpus_na = matutils.Sparse2Corpus(sparse_counts_na)\n",
    "\n",
    "# Gensim also requires a dictionary of the all terms and their respective location in the TDM\n",
    "id2word_na = dict((v, k) for k, v in cv_na.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"reason\" + 0.002*\"murder\" + 0.001*\"interest\" + 0.001*\"success\" + 0.001*\"hand\" + 0.001*\"point\" + 0.001*\"fear\" + 0.001*\"mean\" + 0.001*\"offic\" + 0.001*\"wonder\"'),\n",
       " (1,\n",
       "  '0.000*\"consider\" + 0.000*\"fear\" + 0.000*\"offic\" + 0.000*\"feel\" + 0.000*\"murder\" + 0.000*\"sound\" + 0.000*\"field\" + 0.000*\"line\" + 0.000*\"reason\" + 0.000*\"busi\"'),\n",
       " (2,\n",
       "  '0.000*\"reason\" + 0.000*\"sound\" + 0.000*\"offic\" + 0.000*\"success\" + 0.000*\"murder\" + 0.000*\"mean\" + 0.000*\"busi\" + 0.000*\"hand\" + 0.000*\"arm\" + 0.000*\"ground\"'),\n",
       " (3,\n",
       "  '0.000*\"interest\" + 0.000*\"reason\" + 0.000*\"murder\" + 0.000*\"fear\" + 0.000*\"mean\" + 0.000*\"feel\" + 0.000*\"leg\" + 0.000*\"featur\" + 0.000*\"mark\" + 0.000*\"line\"'),\n",
       " (4,\n",
       "  '0.000*\"roll\" + 0.000*\"reason\" + 0.000*\"nantucket\" + 0.000*\"feel\" + 0.000*\"live\" + 0.000*\"consider\" + 0.000*\"mountain\" + 0.000*\"cri\" + 0.000*\"term\" + 0.000*\"oar\"'),\n",
       " (5,\n",
       "  '0.006*\"blade\" + 0.004*\"hempen\" + 0.004*\"creami\" + 0.004*\"pacif\" + 0.004*\"hereaft\" + 0.004*\"rack\" + 0.003*\"rig\" + 0.003*\"veri\" + 0.003*\"wretch\" + 0.003*\"larboard\"'),\n",
       " (6,\n",
       "  '0.000*\"reason\" + 0.000*\"murder\" + 0.000*\"hand\" + 0.000*\"mean\" + 0.000*\"matter\" + 0.000*\"gener\" + 0.000*\"use\" + 0.000*\"ear\" + 0.000*\"wood\" + 0.000*\"pass\"'),\n",
       " (7,\n",
       "  '0.000*\"murder\" + 0.000*\"offic\" + 0.000*\"reason\" + 0.000*\"success\" + 0.000*\"consider\" + 0.000*\"dark\" + 0.000*\"sound\" + 0.000*\"tree\" + 0.000*\"mark\" + 0.000*\"one\"'),\n",
       " (8,\n",
       "  '0.000*\"feel\" + 0.000*\"rib\" + 0.000*\"live\" + 0.000*\"wonder\" + 0.000*\"fear\" + 0.000*\"point\" + 0.000*\"reason\" + 0.000*\"sound\" + 0.000*\"success\" + 0.000*\"mysteri\"'),\n",
       " (9,\n",
       "  '0.000*\"reason\" + 0.000*\"murder\" + 0.000*\"interest\" + 0.000*\"point\" + 0.000*\"window\" + 0.000*\"sound\" + 0.000*\"case\" + 0.000*\"consider\" + 0.000*\"mean\" + 0.000*\"pass\"')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of topics and the number of passes for LDA\n",
    "lda_na = models.LdaModel(corpus=corpus_na, id2word=id2word_na, num_topics=10, passes=10)\n",
    "lda_na.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'TheOldManAndTheSea'),\n",
       " (0, 'TheAdventuresofSherlock Holmes'),\n",
       " (0, 'MobyDick')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = lda_na[corpus_na]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtm_na.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA (Latent Direchlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = nltk.sent_tokenize(txt)\n",
    "\n",
    "# Stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 't', 'edu', 'use'])\n",
    "\n",
    "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 2 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df=2, max_df=0.2, stop_words=stop_words, \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b').fit(sentences)\n",
    "\n",
    "# Transform into DTM\n",
    "X = vect.transform(sentences)\n",
    "\n",
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "stemmed_words = [porter.stem(w) for w in sentences]\n",
    "# stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id_map,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id_map)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- [Tutorial: Create an Azure Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup)\n",
    "\n",
    "- [Tutorial: Deploy an image classification model in Azure Container Instances](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-deploy-models-with-aml)\n",
    "\n",
    "- [Model Deployment Notebook](https://drive.google.com/file/d/1zxXOBhTTUCyGSGu3HEqbxVQ4WYLEEkGp/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## [Regression,  Multiclass classification](https://github.com/jeffheaton/t81_558_deep_learning/blob/cee8fcc5afedbee9536f52ef9725d67cf1584952/t81_558_class_03_2_keras.ipynb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## [Early Stopping](https://github.com/jeffheaton/t81_558_deep_learning/blob/cee8fcc5afedbee9536f52ef9725d67cf1584952/t81_558_class_03_4_early_stop.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using model output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Transfer Learning using VGG16 Notebook](https://drive.google.com/file/d/1l6WaDZcSXmI9VoWsBG1aOwgDdFdu0nTi/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "num_classes = len(labels)\n",
    "\n",
    "vgg16_model = VGG16(include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = vgg16_model.output\n",
    "x = Flatten()(x)    \n",
    "x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(vgg16_model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_preproc = preprocess_input(X_train)\n",
    "X_test_preproc = preprocess_input(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_1hot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_1hot))\n",
    "score = model.evaluate(X_test, y_test_1hot, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Plot Training & Validation Loss\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sequential layering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "vgg16_model = VGG16()\n",
    "# vgg16_model.summary()\n",
    "\n",
    "model = Sequential()\n",
    "input_shape = [None, 224, 224, 3]\n",
    "\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model.add(layer)\n",
    "\n",
    "model.build(input_shape)\n",
    "\n",
    "# model.layers.pop()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.add (Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Feed Forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_hist(history):\n",
    "  history_dict=history.history\n",
    "  loss_values = history_dict['loss']\n",
    "  val_loss_values=history_dict['val_loss']\n",
    "  plt.figure(figsize=(10,6))\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.plot(loss_values, color='Blue', linestyle='dashed', marker='o', label='Training Loss')\n",
    "  plt.plot(val_loss_values,color='Red', label='Validation Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def model1(X, y):\n",
    "  name = 'Model 1'\n",
    "\n",
    "  input_shape = X[0].shape\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Dense(256, input_shape=input_shape, activation='softmax'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(1, activation='linear'))\n",
    "  model.compile(optimizer='adam', \n",
    "                loss='mean_absolute_error',\n",
    "                metrics=['mean_absolute_error'])\n",
    "\n",
    "  # Runs model and assigns it to 'history'\n",
    "  history = model.fit(X, y, epochs = 200, batch_size=32, validation_split = 0.2, verbose=0)\n",
    "\n",
    "  y_pred = model.predict(X)\n",
    "  print(f'{name}: The R2 score on the Train set is:\\t{r2_score(y, y_pred):.3f}')\n",
    "  print(f'{name}: The Mean Absolute Error on the Train set is:\\t{mean_absolute_error(y, y_pred):.3f}')\n",
    "  \n",
    "  # Plot Training & Validation Loss\n",
    "  plot_hist(history)\n",
    "  \n",
    "  return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = scale('standard', df3.iloc[:,:-1])\n",
    "y = df3.iloc[:,-1]\n",
    "iters = 1\n",
    "\n",
    "for iter in range(iters):\n",
    "  model_1 = model1(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Saving & Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "# with open('../data/models/lgbm1.joblib', 'wb') as joblibfile:\n",
    "#     joblib.dump(lgbm, joblibfile)\n",
    "    \n",
    "# with open('../data/models/lgbm1.joblib', 'rb') as joblibfile:\n",
    "#     lgbm = joblib.load(joblibfile)\n",
    "\n",
    "\n",
    "# with open('../data/models/lgbm1.pkl', 'wb') as pklfile:\n",
    "#     pickle.dump(lgbm, pklfile)\n",
    "    \n",
    "with open('../data/models/lgbm1.pkl', 'rb') as pklfile:\n",
    "    lgbm = pickle.load(pklfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[YAML, JSON, HDF5 formats](https://github.com/jeffheaton/t81_558_deep_learning/blob/cee8fcc5afedbee9536f52ef9725d67cf1584952/t81_558_class_03_3_save_load.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "sn.heatmap(cm_tree, annot=True)\n",
    "plt.title(f'Confusion Matrix - Decision tree\\nTest data score = {tree_score*100:.2f}%')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corr = df3.corr(method='pearson')\n",
    "corr.style.background_gradient(cmap='coolwarm')\n",
    "pd.DataFrame(corr).transpose().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for feature in corr['winner'].iteritems():\n",
    "  if ((feature[1] >= 0.1) or (feature[1] <= -0.1)):  # Features with +ve or -ve correlation with 'winner' of > 10% \n",
    "    print (feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df3.iloc[:,:-1]\n",
    "y = df3.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective='binary', metric='binary_logloss', n_estimators=100, num_leaves=10,\n",
    "      )\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_score = lgbm.score(X_test, y_test)\n",
    "print (f'LightGBM score on test data = {lgbm_score*100:.2f}%')\n",
    "\n",
    "y_pred_lgbm = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "def feature_imp(clf, X):\n",
    "  threshold = 0.5\n",
    "  feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "  feature_imp_sorted = feature_imp.sort_values(by=\"Value\", ascending=False)\n",
    "\n",
    "  print (f'Threshold = {threshold}')\n",
    "  print (feature_imp_sorted[feature_imp_sorted['Value'] >= threshold])\n",
    "  plt.figure(figsize=(15, 12))\n",
    "  sn.barplot(x=\"Value\", y=\"Feature\", data=feature_imp_sorted)\n",
    "  plt.title(type(clf))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feature_imp(lgbm, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
